import torch
import time
from transformers import AutoModelForImageTextToText, AutoProcessor
from peft import PeftModel
import gradio as gr

# =========================
# 1. æ¨¡å‹åŠ è½½
# =========================
print("æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹ï¼Œè¯·ç¨å€™...")

BASE_MODEL_PATH = "/root/autodl-fs/Qwen2.5-VL-7B-Instruct"
LORA_PATH = "/root/autodl-tmp/coda-lm-llava-format/output_adaqlora"

print("åŠ è½½åŸºç¡€æ¨¡å‹...")
base_model = AutoModelForImageTextToText.from_pretrained(
    BASE_MODEL_PATH,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

print("åŠ è½½ LoRA...")
model = PeftModel.from_pretrained(base_model, LORA_PATH)
model.eval()

print("åŠ è½½ Processor...")
try:
    processor = AutoProcessor.from_pretrained(LORA_PATH, trust_remote_code=True)
    print("OK!")
except:
    processor = AutoProcessor.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)

print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

# =========================
# 2. é»˜è®¤ Prompt
# =========================
DEFAULT_PROMPT = ""

# =========================
# 3. æ¨ç†å‡½æ•°ï¼ˆåªè´Ÿè´£æ¨¡å‹ï¼‰
# =========================
def generate_response(image, text):
    messages = [{
        "role": "user",
        "content": [
            {"type": "image", "image": image},
            {"type": "text", "text": text}
        ]
    }]

    prompt = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    inputs = processor(
        text=[prompt],
        images=[image],
        return_tensors="pt",
        padding=True
    ).to(model.device)

    start = time.time()
    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=512,
            do_sample=False
        )

    output_ids = output_ids[:, inputs.input_ids.shape[1]:]
    result = processor.batch_decode(
        output_ids,
        skip_special_tokens=True
    )[0]

    return result + f"\n\nâ± æ¨ç†è€—æ—¶ {time.time() - start:.2f}s"

# =========================
# 4. Gradio Chat é€»è¾‘ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼‰
# =========================
def chat(image, text, history):
    history = history or []

    # ---------- å‰ç½®æ ¡éªŒ ----------
    if image is None:
        history.append(["âŒ è¾“å…¥é”™è¯¯", "è¯·å…ˆä¸Šä¼ ä¸€å¼ å›¾ç‰‡"])
        yield history, ""
        return

    if not text or text.strip() == "":
        history.append(["âŒ è¾“å…¥é”™è¯¯", "è¯·è¾“å…¥é—®é¢˜æ–‡æœ¬"])
        yield history, ""
        return

    # ---------- â‘  ç«‹åˆ»æ˜¾ç¤ºç”¨æˆ·é—®é¢˜ ----------
    history.append([text, "ğŸ¤– æ­£åœ¨ç”Ÿæˆä¸­ï¼Œè¯·ç¨å€™..."])
    yield history, ""

    # ---------- â‘¡ æ‰§è¡Œæ¨¡å‹æ¨ç† ----------
    answer = generate_response(image, text)

    # ---------- â‘¢ æ›´æ–°æœ€åä¸€æ¡å›ç­” ----------
    history[-1][1] = answer
    yield history, ""

def clear():
    return None, "", []

# =========================
# 5. Gradio UI
# =========================
with gr.Blocks(title="Qwen2.5-VL Traffic Assistant") as demo:
    gr.Markdown("# ğŸš— Qwen2.5-VL äº¤é€šåœºæ™¯ç†è§£")

    with gr.Row():
        image = gr.Image(type="pil", label="è¾“å…¥å›¾åƒ", height=400)
        chatbot = gr.Chatbot(height=400)

    text = gr.Textbox(
        lines=5,
        value=DEFAULT_PROMPT,
        label="æé—®",
        placeholder="è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆå¿…é¡»å¡«å†™ï¼‰"
    )

    with gr.Row():
        submit = gr.Button("å‘é€")
        clear_btn = gr.Button("æ¸…ç©º")

    submit.click(
        chat,
        inputs=[image, text, chatbot],
        outputs=[chatbot, text]
    )

    clear_btn.click(clear, [], [image, text, chatbot])

# =========================
# 6. å¯åŠ¨
# =========================
if __name__ == "__main__":
    demo.queue()  # ğŸ”¥ å…è®¸ yield / æµå¼æ›´æ–°
    demo.launch(
        server_name="0.0.0.0",
        server_port=6006,
        share=False,
        show_api=False,
        inbrowser=False
    )
